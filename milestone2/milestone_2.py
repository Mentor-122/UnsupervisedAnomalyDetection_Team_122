# -*- coding: utf-8 -*-
"""Milestone 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q3Z9laJQ12W2NLBBbIbRg2yG8tVRnqQV

#start hear
"""

import pandas as pd
from sklearn.preprocessing import LabelEncoder
from sklearn.ensemble import IsolationForest
import matplotlib.pyplot as plt
file_path = '/content/drive/MyDrive/Healthcare Providers.csv'
df = pd.read_csv(file_path)

df.isnull().sum()

df.isnull().sum().sum()

"""#Filling the Null values with some values"""

df = df.fillna({'Credentials of the Provider':'NA',
                'Gender of the Provider':'NA'})

"""#Filling the Null values by before values"""

df = df.fillna({'First Name of the Provider' : 'NA'})
df= df.fillna(method = 'pad',axis = 1)
df

df.isnull().sum()

df['Credentials of the Provider'] = df['Credentials of the Provider'].str.replace('.', '', regex=False)
df

# Remove commas from numerical columns
numerical_columns = [
    'Number of Services',
    'Number of Medicare Beneficiaries',
    'Number of Distinct Medicare Beneficiary/Per Day Services',
    'Average Medicare Allowed Amount',
    'Average Submitted Charge Amount',
    'Average Medicare Payment Amount',
    'Average Medicare Standardized Amount'
]

for col in numerical_columns:
    df[col] = df[col].str.replace(',', '').astype(float)

df



"""Here we scaling down the feature

• In the graph above 3 and below 3 values the density is not going to "0"

• In logistic function it is slowly and asintotically get's closer to "0"

#Encoding
"""

from sklearn.preprocessing import LabelEncoder

df = pd.read_csv('/content/drive/MyDrive/Healthcare Providers.csv', encoding='ascii')

# Perform label encoding
label_columns = ['Gender of the Provider', 'Entity Type of the Provider', 'Country Code of the Provider',
                     'HCPCS Drug Indicator', 'Medicare Participation Indicator', 'Place of Service']

label_encoders = {}
def label_encoding(df, columns):
  for col in label_columns:
    le = LabelEncoder()
    df[col] = le.fit_transform(df[col])
    label_encoders[col] = le
label_encoding(df,label_columns)

# Perform frequency encoding
freq_columns = ['Provider Type', 'State Code of the Provider', 'Credentials of the Provider', 'HCPCS Code']
def frequency_encoding(df, columns):
    for col in columns:
        freq_enc = df[col].value_counts().to_dict()
        df[col + '_freq'] = df[col].map(freq_enc)
frequency_encoding(df,freq_columns)
df

import pandas as pd


# Define categorical columns
categorical_columns = ['Gender of the Provider', 'Entity Type of the Provider', 'Country Code of the Provider',
                     'HCPCS Drug Indicator', 'Medicare Participation Indicator', 'Place of Service']

# Perform one-hot encoding for categorical columns with high cardinality
high_cardinality_columns = ['Provider Type', 'State Code of the Provider', 'Credentials of the Provider', 'HCPCS Code']
df = pd.get_dummies(df, columns=high_cardinality_columns)

# Perform frequency encoding for categorical columns with low cardinality
low_cardinality_columns = ['Gender of the Provider', 'Entity Type of the Provider', 'Country Code of the Provider',
                     'HCPCS Drug Indicator', 'Medicare Participation Indicator', 'Place of Service']

for col in low_cardinality_columns:
    freq_enc = df[col].value_counts().to_dict()
    df[col + '_freq'] = df[col].map(freq_enc)

# Print the encoded DataFrame
print(df.head())

"""#new columns"""

df['Full Name'] = df['Last Name/Organization Name of the Provider'].fillna('') + ' ' + \
                  df['First Name of the Provider'].fillna('') + ' ' + \
                  df['Middle Initial of the Provider'].fillna('')


df['Full Name'] = df['Full Name'].str.strip()

print(df[['Full Name']].head())

"""By combining the last name,first name and middle name this column has been added."""

numerical_columns = [
    'Number of Services',
    'Number of Medicare Beneficiaries',
    'Number of Distinct Medicare Beneficiary/Per Day Services',
    'Average Medicare Allowed Amount',
    'Average Submitted Charge Amount',
    'Average Medicare Payment Amount',
    'Average Medicare Standardized Amount'
]

for col in numerical_columns:
    df[col] = df[col].str.replace(',', '').astype(float)
df['service_to_beneficiary_ratio'] =  df['Number of Medicare Beneficiaries']/df['Number of Services']

print(df[['service_to_beneficiary_ratio']].head())

"""Through this ratio we can find the  Quality of Care Assessment.

This has be created by dividing the Number of Medicare Beneficiaries and Number of Services they provided.


"""

low_cardinality_columns = ['Gender of the Provider',
    'Country Code of the Provider',
    'Medicare Participation Indicator',
    'Place of Service',
    'Entity Type of the Provider']

for col in low_cardinality_columns:
    freq_enc = df[col].value_counts().to_dict()
    df[col + '_freq'] = df[col].map(freq_enc)

df['Average Submitted Charge Amount'] = pd.to_numeric(df['Average Submitted Charge Amount'], errors='coerce')
df['Average Medicare Payment Amount'] = pd.to_numeric(df['Average Medicare Payment Amount'], errors='coerce')

# Calculate the ratio
df['Charge to Payment Ratio'] = df['Average Submitted Charge Amount'] / df['Average Medicare Payment Amount']


df

"""The "Charge to Payment Ratio column" is created by dividing the 'Average Submitted Charge Amount' by the 'Average Medicare Payment Amount'.

This has been create to the Evaluate Cost Efficiency.
"""

# prompt: pront coliumns

print(df.columns)

"""#Normalization

"""

import numpy as np
import matplotlib.pyplot as plt
def logistic(x):
  return 1.0/(1+np.exp(-x))
x = np.linspace(-6,6,1000)
y = logistic(x)
plt.plot(x,y)
plt.show()

"""Here we scaling down the feature between 0 to 1"""

print(df.columns)

"""• In the above dataset we are having 6
numerical columns and I have just done Normalization for 4 numerical columns.

• Normalization applyed on the Number of Average Medicare Allowed Amount, Average Submitted Charge Amount, Average Medicare Payment Amount, and Average Medicare Standardized Amount.

• Here the maximum value is 1 and the minimum value is 0.

•Insted of using minmax scaller we use Standardization caues in it is very effective to find the outliers than this.

#Standardization

• In Standardization the features will be transformed in such a way that will have the properties of a standard normal distribution.

• mean is usually "0" and the standard deviation is "1".
"""

from sklearn.preprocessing import StandardScaler
import pandas as pd
# Remove commas from numerical columns
numerical_columns = [
    'Number of Services',
    'Number of Medicare Beneficiaries',
    'Number of Distinct Medicare Beneficiary/Per Day Services',
    'Average Medicare Allowed Amount',
    'Average Submitted Charge Amount',
    'Average Medicare Payment Amount',
    'Average Medicare Standardized Amount'
]
for col in numerical_columns:
    if df[col].dtype == 'object':  # Check if the column is of object type (likely string)
        df[col] = df[col].str.replace(',', '').astype(float)
scaler = StandardScaler()

x_scaled = scaler.fit_transform(df[[
       'Average Medicare Allowed Amount', 'Average Submitted Charge Amount',
       'Average Medicare Payment Amount',
       'Average Medicare Standardized Amount']])

print(x_scaled)

#mean value
np.mean(x_scaled)

# varience
np.var(x_scaled)

"""# PCA"""

import pandas as pd
from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.decomposition import IncrementalPCA
from sklearn.impute import SimpleImputer
import numpy as np
import matplotlib.pyplot as plt

# Load and preprocess the data
df_cleaned = df.copy()

# Identify categorical columns
categorical_columns = df_cleaned.select_dtypes(include=['object']).columns

# Separate categorical columns based on cardinality
low_cardinality_cols = [col for col in categorical_columns if df_cleaned[col].nunique() <= 10]
high_cardinality_cols = [col for col in categorical_columns if df_cleaned[col].nunique() > 10]

# One-hot encode low cardinality columns
onehot_encoder = OneHotEncoder(drop='first', sparse=False)
onehot_encoded = onehot_encoder.fit_transform(df_cleaned[low_cardinality_cols])
onehot_encoded_df = pd.DataFrame(onehot_encoded, columns=onehot_encoder.get_feature_names_out(low_cardinality_cols))

# Frequency encode high cardinality columns
for col in high_cardinality_cols:
    freq_encoding = df_cleaned[col].value_counts() / len(df_cleaned)
    df_cleaned[col] = df_cleaned[col].map(freq_encoding)

# Drop the original low cardinality categorical columns as they are now encoded
df_cleaned = df_cleaned.drop(columns=low_cardinality_cols)

# Concatenate the one-hot encoded columns with the cleaned dataframe
df_encoded = pd.concat([df_cleaned.reset_index(drop=True), onehot_encoded_df], axis=1)

# Identify numerical columns for scaling
numerical_columns = df_encoded.select_dtypes(include=[np.number]).columns

# Apply StandardScaler to the numerical columns
scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_encoded[numerical_columns])
scaled_df = pd.DataFrame(scaled_data, columns=numerical_columns)

# Combine scaled data with non-numerical columns (which are already encoded and scaled)
final_df = pd.concat([scaled_df.reset_index(drop=True), df_encoded.drop(columns=numerical_columns).reset_index(drop=True)], axis=1)

# Check and handle infinite values
final_df.replace([np.inf, -np.inf], np.nan, inplace=True)

# Impute missing values (NaNs) with the mean of each column in chunks
def impute_in_chunks(dataframe, chunk_size=1000):
    imputer = SimpleImputer(strategy='mean')
    imputed_data = []

    for start in range(0, dataframe.shape[0], chunk_size):
        end = min(start + chunk_size, dataframe.shape[0])
        chunk = dataframe.iloc[start:end]
        imputed_chunk = imputer.fit_transform(chunk)
        imputed_data.append(imputed_chunk)

    return np.vstack(imputed_data)

final_df_imputed = pd.DataFrame(impute_in_chunks(final_df), columns=final_df.columns)

# Ensure the number of components does not exceed the batch size
n_samples = final_df_imputed.shape[0]
n_features = final_df_imputed.shape[1]
n_components = min(n_samples, n_features, 500)  # Ensure n_components <= batch_size and reasonable value

incremental_pca = IncrementalPCA(n_components=n_components, batch_size=1000)
incremental_pca.fit(final_df_imputed)

# Calculate cumulative variance explained
cumulative_variance_explained = np.cumsum(incremental_pca.explained_variance_ratio_)

# Plot cumulative variance explained
plt.figure(figsize=(10, 6))
plt.plot(range(1, len(cumulative_variance_explained) + 1), cumulative_variance_explained, marker='o', linestyle='--')
plt.title('Cumulative Variance Explained by PCA Components')
plt.xlabel('Number of Principal Components')
plt.ylabel('Cumulative Variance Explained')
plt.grid(True)
plt.show()

from sklearn.preprocessing import StandardScaler, OneHotEncoder
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline

categorical_cols = ['Credentials of the Provider', 'Gender of the Provider', 'Entity Type of the Provider', 'City of the Provider', 'State Code of the Provider', 'Country Code of the Provider', 'Provider Type', 'Medicare Participation Indicator', 'Place of Service', 'HCPCS Drug Indicator']
numerical_cols = ['Number of Services', 'Number of Medicare Beneficiaries', 'Average Medicare Allowed Amount', 'Average Submitted Charge Amount', 'Average Medicare Payment Amount', 'Average Medicare Standardized Amount']


categorical_transformer = Pipeline(steps=[
    ('onehot', OneHotEncoder(handle_unknown='ignore'))
])

numerical_transformer = Pipeline(steps=[
    ('scaler', StandardScaler())
])

numerical_columns = [
    'Number of Services',
    'Number of Medicare Beneficiaries',
    'Number of Distinct Medicare Beneficiary/Per Day Services',
    'Average Medicare Allowed Amount',
    'Average Submitted Charge Amount',
    'Average Medicare Payment Amount',
    'Average Medicare Standardized Amount'
]

# Combine numerical and encoded categorical columns
combined_columns = numerical_columns + list(df.filter(like='_freq').columns)

# Apply StandardScaler to combined columns
scaler = StandardScaler()
x_scaled = scaler.fit_transform(df[combined_columns])

# Print the scaled data
print(x_scaled)

import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
import seaborn as sns


num_clusters = 3

# Initialize the KMeans model
kmeans = KMeans(n_clusters=num_clusters, random_state=0)
numerical_columns = [
    'Number of Services',
    'Number of Medicare Beneficiaries',
    'Number of Distinct Medicare Beneficiary/Per Day Services',
    'Average Medicare Allowed Amount',
    'Average Submitted Charge Amount',
    'Average Medicare Payment Amount',
    'Average Medicare Standardized Amount'
]

kmeans.fit(df[numerical_cols])

# Get the cluster labels
labels = kmeans.labels_

# Add the cluster labels to the DataFrame
df['Cluster'] = labels

# Print the cluster centers
print(kmeans.cluster_centers_)

# Visualize the clusters
sns.scatterplot(x='Number of Services', y='Average Medicare Payment Amount', hue='Cluster', data=df)
plt.show()

from sklearn.cluster import KMeans
import numpy as np
from sklearn.impute import SimpleImputer

num_clusters = 3

kmeans = KMeans(n_clusters=num_clusters, random_state=0)

imputer = SimpleImputer(strategy='mean')
x_scaled_imputed = imputer.fit_transform(x_scaled)

kmeans.fit(x_scaled_imputed)

labels = kmeans.labels_

df['Cluster'] = labels

print(kmeans.cluster_centers_)

import matplotlib.pyplot as plt

plt.scatter(x_scaled_imputed[:, 0], x_scaled_imputed[:, 1], c=labels, cmap='viridis')
plt.title('Clusters of Healthcare Providers')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

wcss = []

for i in range(2, 6):
    kmeans = KMeans(n_clusters=i, random_state=0)

    # Fit the model to the data
    kmeans.fit(x_scaled_imputed)

    wcss.append(kmeans.inertia_)

plt.plot(range(2, 6), wcss)
plt.title('Elbow Plot')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS')
plt.show()

optimal_k = 3

kmeans = KMeans(n_clusters=optimal_k, random_state=0)

# Fit the model to the data
kmeans.fit(x_scaled_imputed)

df['Cluster'] = kmeans.labels_

print(kmeans.cluster_centers_)

# Visualize the clusters
plt.scatter(x_scaled_imputed[:, 0], x_scaled_imputed[:, 1], c=labels, cmap='viridis')
plt.title('Clusters of Healthcare Providers')
plt.xlabel('PC1')
plt.ylabel('PC2')
plt.show()

import pandas as pd
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler

# Selecting relevant numerical columns for k-means clustering
numerical_columns = [
    'Number of Services',
    'Number of Medicare Beneficiaries',
    'Number of Distinct Medicare Beneficiary/Per Day Services',
    'Average Medicare Allowed Amount',
    'Average Submitted Charge Amount',
    'Average Medicare Payment Amount',
    'Average Medicare Standardized Amount'
]

# Extract the data for these columns from 'df'
data_for_clustering = df[numerical_columns]

# Handle any missing values by filling them with the mean of the column
data_for_clustering.fillna(data_for_clustering.mean(), inplace=True)

# Standardize the data before clustering
scaler = StandardScaler()
data_scaled = scaler.fit_transform(data_for_clustering)

# Perform k-means clustering with a chosen number of clusters, e.g., 3
kmeans = KMeans(n_clusters=3, random_state=42)

# Assign the clusters to the original dataframe 'df'
df['Cluster'] = kmeans.fit_predict(data_scaled)

# Show the number of data points in each cluster
df['Cluster'].value_counts()

import pandas as pd
pd.DataFrame(data_scaled).head().T